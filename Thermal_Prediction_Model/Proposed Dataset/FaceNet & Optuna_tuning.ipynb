{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ca91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# === CONFIG: Optimal Hyperparameters Found by Tuning ===\n",
    "BEST_PARAMS = {\n",
    "    \"lr\": 0.005262345849736729,\n",
    "    \"n_layers\": 3,\n",
    "    \"n_units_l0\": 472,\n",
    "    \"dropout_l0\": 0.4336111047085437,\n",
    "    \"n_units_l1\": 105,\n",
    "    \"dropout_l1\": 0.37264881799115585,\n",
    "    \"n_units_l2\": 399,\n",
    "    \"dropout_l2\": 0.32824162595518847,\n",
    "}\n",
    "\n",
    "EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# === Load Pre-computed Features ===\n",
    "print(\"ðŸš€ Loading pre-computed features...\")\n",
    "try:\n",
    "    feature_data = torch.load('extracted_features.pth')\n",
    "    embeddings = feature_data['embeddings']\n",
    "    labels = feature_data['labels']\n",
    "    groups = feature_data['groups']\n",
    "    class_names = feature_data['class_names']\n",
    "    print(\"âœ… Features loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Error: 'extracted_features.pth' not found. Please run the Step 1 feature extraction script first.\")\n",
    "    exit()\n",
    "\n",
    "# === Build the Optimal Classifier ===\n",
    "layers = []\n",
    "in_features = embeddings.shape[1]\n",
    "for i in range(BEST_PARAMS[\"n_layers\"]):\n",
    "    out_features = BEST_PARAMS[f\"n_units_l{i}\"]\n",
    "    layers.append(nn.Linear(in_features, out_features))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(BEST_PARAMS[f\"dropout_l{i}\"]))\n",
    "    in_features = out_features\n",
    "\n",
    "layers.append(nn.Linear(in_features, len(class_names)))\n",
    "classifier = nn.Sequential(*layers).to(DEVICE)\n",
    "print(\"\\nOptimal classifier architecture built.\")\n",
    "\n",
    "# === K-Fold Cross-Validation ===\n",
    "skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_accuracies = []\n",
    "\n",
    "# Variables to store results from the best fold\n",
    "best_fold_acc = 0.0\n",
    "best_fold_history = None\n",
    "best_fold_model_state = None\n",
    "best_fold_y_true = None\n",
    "best_fold_y_pred = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(embeddings, labels, groups)):\n",
    "    print(f\"\\n{'='*20} FOLD {fold+1}/5 {'='*20}\\n\")\n",
    "    \n",
    "    train_dataset = TensorDataset(embeddings[train_idx], labels[train_idx])\n",
    "    val_dataset = TensorDataset(embeddings[val_idx], labels[val_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    for layer in classifier.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=BEST_PARAMS[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # History for the current fold\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    best_acc_in_fold = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # --- Training ---\n",
    "        classifier.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == y_batch).sum().item()\n",
    "            train_total += y_batch.size(0)\n",
    "        \n",
    "        history['train_loss'].append(train_loss / train_total)\n",
    "        history['train_acc'].append(train_correct / train_total)\n",
    "\n",
    "        # --- Validation ---\n",
    "        classifier.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        current_preds, current_trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                outputs = classifier(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.size(0)\n",
    "                current_preds.extend(preds.cpu().numpy())\n",
    "                current_trues.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        history['val_loss'].append(val_loss / val_total)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Checkpoint and Early Stopping\n",
    "        if val_acc > best_acc_in_fold:\n",
    "            best_acc_in_fold = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_in_fold_state = copy.deepcopy(classifier.state_dict())\n",
    "            # Store predictions from the best epoch of this fold\n",
    "            best_preds_in_fold = current_preds\n",
    "            best_trues_in_fold = current_trues\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                print(f\"Epoch {epoch+1}: Early stopping triggered.\")\n",
    "                break\n",
    "                \n",
    "    print(f\"ðŸ“ˆ Best accuracy for fold {fold+1}: {best_acc_in_fold*100:.2f}%\")\n",
    "    fold_accuracies.append(best_acc_in_fold)\n",
    "    \n",
    "    # Check if this fold was the best one overall\n",
    "    if best_acc_in_fold > best_fold_acc:\n",
    "        best_fold_acc = best_acc_in_fold\n",
    "        best_fold_history = history\n",
    "        best_fold_y_true = best_trues_in_fold\n",
    "        best_fold_y_pred = best_preds_in_fold\n",
    "\n",
    "# === Final Results ===\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "print(f\"\\n\\n{'='*20} K-FOLD CROSS-VALIDATION COMPLETE {'='*20}\")\n",
    "print(f\"Individual Fold Accuracies: {[f'{acc*100:.2f}%' for acc in fold_accuracies]}\")\n",
    "print(f\"âœ… Average Validation Accuracy: {mean_accuracy*100:.2f}% Â± {std_accuracy*100:.2f}%\")\n",
    "\n",
    "# === Plotting Graphs for the Best Fold ===\n",
    "print(\"\\nðŸ“Š Generating visualizations for the best performing fold...\")\n",
    "plt.style.use('dark_background')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plotting Loss\n",
    "ax1.plot(best_fold_history['train_loss'], label='Train Loss')\n",
    "ax1.plot(best_fold_history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss per Epoch')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "ax2.plot(best_fold_history['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(best_fold_history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy per Epoch')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plotting Confusion Matrix for the Best Fold ===\n",
    "cm = confusion_matrix(best_fold_y_true, best_fold_y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Confusion Matrix for Best Fold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
